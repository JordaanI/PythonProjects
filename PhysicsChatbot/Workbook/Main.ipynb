{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "### UI SHELL ###\n",
    "################\n",
    "\n",
    "# from tkinter import *\n",
    "# root = Tk()\n",
    "# root.title(\"Chatbot\")\n",
    "# def send():\n",
    "#     send = \"You -> \"+e.get()\n",
    "#     txt.insert(END, \"n\"+send)\n",
    "#     user = e.get().lower()\n",
    "#     if(user == \"hello\"):\n",
    "#         txt.insert(END, \"n\" + \"Bot -> Hi\")\n",
    "#     elif(user == \"hi\" or user == \"hii\" or user == \"hiiii\"):\n",
    "#         txt.insert(END, \"n\" + \"Bot -> Hello\")\n",
    "#     elif(e.get() == \"how are you\"):\n",
    "#         txt.insert(END, \"n\" + \"Bot -> fine! and you\")\n",
    "#     elif(user == \"fine\" or user == \"i am good\" or user == \"i am doing good\"):\n",
    "#         txt.insert(END, \"n\" + \"Bot -> Great! how can I help you.\")\n",
    "#     else:\n",
    "#         txt.insert(END, \"n\" + \"Bot -> Sorry! I dind't got you\")\n",
    "#     e.delete(0, END)\n",
    "# txt = Text(root)\n",
    "# txt.grid(row=0, column=0, columnspan=2)\n",
    "# e = Entry(root, width=100)\n",
    "# e.grid(row=1, column=0)\n",
    "# send = Button(root, text=\"Send\", command=send).grid(row=1, column=1)\n",
    "# root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "### IMPORTS ###\n",
    "###############\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, TimeDistributed, Dense\n",
    "from tensorflow.keras.layers import Embedding, GRU\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### Functions ###\n",
    "#################\n",
    "\n",
    "def reformat_text(text):\n",
    "    text = re.sub(r'\\(.+?\\)', '', text)\n",
    "    text = re.sub(r'\\{.+?\\}', '', text)\n",
    "    text = re.sub(r'[-_()0-9%$:\\^\\/°\\∼\\~π]', '', text)\n",
    "    text = re.sub(r'(\\.)\\1+','', text)\n",
    "    text = re.sub(r'\\w*<sub>','', text)\n",
    "    text = re.sub(r'\\$.+?\\$', '', text)\n",
    "    text = re.sub(r'<.+?>','',text)\n",
    "    text = re.sub(r'[<>]', '', text)\n",
    "    text = re.sub(r'(\\s)\\1+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def lookup(param):\n",
    "    if type(param) is int:\n",
    "        return word_index[param]\n",
    "    elif type(param) is str:\n",
    "        return word_lexicon[param]\n",
    "    else:\n",
    "        print('Parameter not accepted.')\n",
    "\n",
    "def shuffle(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def remove_words(arr,lexicon):\n",
    "    arr = arr.split(' ')\n",
    "    for token in arr:\n",
    "        if token not in lexicon:\n",
    "            arr.remove(token)\n",
    "    return arr\n",
    "\n",
    "def create_model(training_shape,label_shape, n_word_embedding_nodes,\n",
    "                 n_tag_embedding_nodes, n_hidden_nodes, stateful=True, batch_size=1):\n",
    "    \n",
    "    #Layers 1\n",
    "    word_input = Input(batch_shape=(batch_size, training_shape[1]), name='word_input_layer')\n",
    "    tag_input = Input(batch_shape=(batch_size, label_shape[1]), name='tag_input_layer')\n",
    "\n",
    "    #Layers 2\n",
    "    word_embeddings = Embedding(input_dim=training_shape[0],\n",
    "                                output_dim=n_word_embedding_nodes, \n",
    "                                mask_zero=True, name='word_embedding_layer')(word_input) #mask_zero will ignore 0 padding\n",
    "    #Output shape = (batch_size, seq_input_len, n_word_embedding_nodes)\n",
    "    tag_embeddings = Embedding(input_dim=label_shape[0],\n",
    "                               output_dim=n_tag_embedding_nodes,\n",
    "                               mask_zero=True, name='tag_embedding_layer')(tag_input) \n",
    "    #Output shape = (batch_size, seq_input_len, n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 3\n",
    "    merged_embeddings = Concatenate(axis=0, name='concat_embedding_layer')([word_embeddings, tag_embeddings])\n",
    "    #Output shape =  (batch_size, seq_input_len, n_word_embedding_nodes + n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 4\n",
    "    hidden_layer = GRU(units=n_hidden_nodes, return_sequences=True, \n",
    "                       stateful=stateful, name='hidden_layer')(merged_embeddings)\n",
    "    #Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "    \n",
    "    #Layer 5\n",
    "    output_layer = TimeDistributed(Dense(units=n_tag_input_nodes, \n",
    "                                         activation='softmax'), name='output_layer')(hidden_layer)\n",
    "    # Output shape = (batch_size, seq_input_len, n_tag_input_nodes)\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "    model = Model(inputs=[word_input, tag_input], outputs=output_layer)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def convert_back(arr):\n",
    "    sentence = []\n",
    "    for token in arr:\n",
    "        sentence.append(lookup(token))\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "def least_used_words(word_counts,min_freq):\n",
    "    delwords = []\n",
    "    for word in word_counts:\n",
    "        if word_counts[word] <= min_freq:\n",
    "            delwords.append(word)\n",
    "    return delwords\n",
    "\n",
    "def make_word_level_model(num_words,\n",
    "                          embedding_matrix,\n",
    "                          lstm_cells=64,\n",
    "                          trainable=False,\n",
    "                          lstm_layers=1,\n",
    "                          bi_direc=False):\n",
    "    \"\"\"Make a word level recurrent neural network with option for pretrained embeddings\n",
    "       and varying numbers of LSTM cell layers.\"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Map words to an embedding\n",
    "    if not trainable:\n",
    "        model.add(\n",
    "            Embedding(\n",
    "                input_dim=num_words,\n",
    "                output_dim=embedding_matrix.shape[1],\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=False,\n",
    "                mask_zero=True))\n",
    "        model.add(Masking())\n",
    "    else:\n",
    "        model.add(\n",
    "            Embedding(\n",
    "                input_dim=num_words,\n",
    "                output_dim=embedding_matrix.shape[1],\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=True))\n",
    "\n",
    "    # If want to add multiple LSTM layers\n",
    "    if lstm_layers > 1:\n",
    "        for i in range(lstm_layers - 1):\n",
    "            model.add(\n",
    "                LSTM(\n",
    "                    lstm_cells,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0.1,\n",
    "                    recurrent_dropout=0.1))\n",
    "\n",
    "    # Add final LSTM cell layer\n",
    "    if bi_direc:\n",
    "        model.add(\n",
    "            Bidirectional(\n",
    "                LSTM(\n",
    "                    lstm_cells,\n",
    "                    return_sequences=False,\n",
    "                    dropout=0.1,\n",
    "                    recurrent_dropout=0.1)))\n",
    "    else:\n",
    "        model.add(\n",
    "            LSTM(\n",
    "                lstm_cells,\n",
    "                return_sequences=False,\n",
    "                dropout=0.1,\n",
    "                recurrent_dropout=0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 4778 samples.\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### Obtaining the Data ###\n",
    "##########################\n",
    "\n",
    "# token = 'ansPyi7MStG33JewfUJsi0SN4wlRyUFqBUMjAj02'\n",
    "# rows = 1 # fetch 1 record at a time\n",
    "# start = 0  # start with the first result\n",
    "# abstracts = []\n",
    "# titles = []\n",
    "\n",
    "# docs = True\n",
    "# while docs:\n",
    "#     result = requests.get(\"https://api.adsabs.harvard.edu/v1/search/query?\" \\\n",
    "#                        \"q=%20abs%3AThermodynamics\" \\\n",
    "#                        \"&sort=date%20desc%2C%20bibcode%20desc\" \\\n",
    "#                        \"&fl=title,abstract\", \\\n",
    "#                        \"&rows={rows}\" \\\n",
    "#                        \"&start={start}\".format(rows=rows,start=start), \\\n",
    "#                        headers={'Authorization': 'Bearer ' + token})\n",
    "# #If the qeury is successful\n",
    "#     if result.status_code == 200:\n",
    "#         docs = result.json()['response']['docs']\n",
    "#         if 'abstract' in docs[0] and 'title' in docs[0]:\n",
    "#             tmp_abstract = [d['abstract'] for d in docs]\n",
    "#             tmp_titles = [d['title'] for d in docs]\n",
    "#             titles = titles + tmp_titles\n",
    "#             abstracts = abstracts + tmp_abstract\n",
    "#     elif result.status_code == 429:\n",
    "#         print(f'Collected {len(titles)} samples.')\n",
    "#         break\n",
    "#     start += rows # increment the start value to move to the next page of result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data = data.iloc[:2500]\n",
    "titles = data['Title']\n",
    "abstracts = data['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "### COPY LISTS ###\n",
    "##################\n",
    "CT = titles[:]\n",
    "CA = abstracts[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>Study the Role of R&lt;SUP&gt;2&lt;/SUP&gt; Term in Cosmol...</td>\n",
       "      <td>It has been shown that AdS/CFT correspondence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>Mechanism and Thermodynamic Characteristics of...</td>\n",
       "      <td>Results are presented from determining the ade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Revealing curvature and stochastic effects on ...</td>\n",
       "      <td>In the theoretical development of normal grain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>Modeling dynamics of the spacecraft power plan...</td>\n",
       "      <td>Modern space programs cover a wide range of mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>First principle calculations of structural, el...</td>\n",
       "      <td>Structural, elastic, electronic and optical pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>Validity and reliability of oral temperature c...</td>\n",
       "      <td>Complex thermodynamics of the human body and e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>Assessing Past and future Hazardous freezing R...</td>\n",
       "      <td>Freezing precipitation, in the form of freezin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>Kerr black hole surrounded by a cloud of strin...</td>\n",
       "      <td>In this paper, an exact solution of the Kerr b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>Continuous cosmic evolution with diffusive bar...</td>\n",
       "      <td>In the background of homogeneous and isotropic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>Entropy-Driven Microstructure Evolution Calcul...</td>\n",
       "      <td>A Potts model and the Replica Exchange Wang-La...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "2139  Study the Role of R<SUP>2</SUP> Term in Cosmol...   \n",
       "2474  Mechanism and Thermodynamic Characteristics of...   \n",
       "174   Revealing curvature and stochastic effects on ...   \n",
       "2204  Modeling dynamics of the spacecraft power plan...   \n",
       "2486  First principle calculations of structural, el...   \n",
       "2439  Validity and reliability of oral temperature c...   \n",
       "2261  Assessing Past and future Hazardous freezing R...   \n",
       "691   Kerr black hole surrounded by a cloud of strin...   \n",
       "1526  Continuous cosmic evolution with diffusive bar...   \n",
       "2332  Entropy-Driven Microstructure Evolution Calcul...   \n",
       "\n",
       "                                               Abstract  \n",
       "2139  It has been shown that AdS/CFT correspondence ...  \n",
       "2474  Results are presented from determining the ade...  \n",
       "174   In the theoretical development of normal grain...  \n",
       "2204  Modern space programs cover a wide range of mi...  \n",
       "2486  Structural, elastic, electronic and optical pr...  \n",
       "2439  Complex thermodynamics of the human body and e...  \n",
       "2261  Freezing precipitation, in the form of freezin...  \n",
       "691   In this paper, an exact solution of the Kerr b...  \n",
       "1526  In the background of homogeneous and isotropic...  \n",
       "2332  A Potts model and the Replica Exchange Wang-La...  "
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "### INSPECT Data ###\n",
    "####################\n",
    "\n",
    "data = {'Title':CT,'Abstract':CA}\n",
    "data = pd.DataFrame(data)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database contains 99.6% unique entries.\n",
      " With 26012 unique words\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### Initial formatting ###\n",
    "##########################\n",
    "\n",
    "#filters='!()\"%;[\\\\]^_`{|}~\\t\\n'\n",
    "if type(data.iloc[0,0]) == list: \n",
    "    data['Title'] = data['Title'].apply(lambda x: x[0])\n",
    "column_values = data[['Title']].values.ravel()\n",
    "unique_values =  pd.unique(column_values)\n",
    "data['Reformatted'] = data['Abstract'].apply(reformat_text)\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(data['Reformatted'])\n",
    "data['Tokens'] = tokenizer.texts_to_sequences(data['Reformatted'])\n",
    "\n",
    "word_lexicon = tokenizer.word_index\n",
    "word_index = tokenizer.index_word\n",
    "num_words = len(word_lexicon) + 1\n",
    "word_counts = tokenizer.word_counts\n",
    "sorted_counts = dict(sorted(dict(word_counts).items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "\n",
    "print(f'The database contains {len(unique_values)/data.shape[0]*100}% unique entries.\\n With {num_words} unique words')\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# x, y = zip(*sorted_counts.items()) # unpack a list of pairs into two tuples\n",
    "# ax.plot(x, y)\n",
    "# ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "# ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "# ax.set_xlabel('Words')\n",
    "# ax.set_ylabel('Word Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database contains 99.6% unique entries.\n",
      " With 12980 unique words\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### Further Formatting ###\n",
    "##########################\n",
    "\n",
    "### Removing the lowest 10% of words.\n",
    "delwords = least_used_words(word_counts=word_counts,min_freq=15)\n",
    "\n",
    "for word in delwords:\n",
    "    del word_lexicon[word]\n",
    "\n",
    "data['Reformatted'] = data['Reformatted'].apply(lambda x : remove_words(x,word_lexicon))\n",
    "\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(data['Reformatted'])\n",
    "data['Tokens'] = tokenizer.texts_to_sequences(data['Reformatted'])\n",
    "\n",
    "word_lexicon = tokenizer.word_index\n",
    "word_index = tokenizer.index_word\n",
    "num_words = len(word_lexicon) + 1\n",
    "word_counts = tokenizer.word_counts\n",
    "sorted_counts = dict(sorted(dict(word_counts).items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "\n",
    "print(f'The database contains {len(unique_values)/data.shape[0]*100}% unique entries.\\n With {num_words} unique words')\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# x, y = zip(*sorted_counts.items()) # unpack a list of pairs into two tuples\n",
    "# ax.plot(x, y)\n",
    "# ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "# ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "# ax.set_xlabel('Words')\n",
    "# ax.set_ylabel('Word Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Reformatted</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cosmological models reconstructed from jerk: A...</td>\n",
       "      <td>The thermodynamic viability of some dark energ...</td>\n",
       "      <td>[The, thermodynamic, of, some, dark, energy, m...</td>\n",
       "      <td>[8, 18, 2, 171, 989, 31, 77, 109, 1, 574, 164,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Investigation into the effect of energy densit...</td>\n",
       "      <td>In this study, the effects of volume energy de...</td>\n",
       "      <td>[In, this, the, effects, of, volume, energy, d...</td>\n",
       "      <td>[25, 23, 1, 118, 2, 337, 31, 70, 14, 5858, 67,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The effects of substrate morphology by regulat...</td>\n",
       "      <td>When cells are cultured on the micro- or nano-...</td>\n",
       "      <td>[When, cells, are, on, the, micro, or, structu...</td>\n",
       "      <td>[713, 1579, 12, 14, 1, 2440, 52, 74, 12, 428, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Holographic dual approach to magnetism and mag...</td>\n",
       "      <td>We propose a dual gravitational theory corresp...</td>\n",
       "      <td>[We, propose, a, dual, gravitational, theory, ...</td>\n",
       "      <td>[17, 353, 5, 2344, 526, 71, 258, 6, 1064, 1065...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Defect induced ferromagnetism in a two-dimensi...</td>\n",
       "      <td>Two-dimensional ferromagnetic materials are po...</td>\n",
       "      <td>[ferromagnetic, materials, are, potential, can...</td>\n",
       "      <td>[1065, 179, 12, 68, 1464, 10, 27, 24, 1624, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>Principles of low dissipation computing from a...</td>\n",
       "      <td>We introduce a thermodynamically consistent, m...</td>\n",
       "      <td>[We, introduce, a, thermodynamically, minimal,...</td>\n",
       "      <td>[17, 707, 5, 220, 1055, 545, 26, 9, 2746, 1928...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>Effect of chaos on the simulation of quantum c...</td>\n",
       "      <td>We study how chaos, introduced by a weak pertu...</td>\n",
       "      <td>[We, study, how, introduced, by, a, weak, affe...</td>\n",
       "      <td>[17, 44, 214, 878, 13, 5, 450, 1114, 1, 2743, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>Non-equilibrium quadratic measurement-feedback...</td>\n",
       "      <td>Measurement and feedback control of thermomech...</td>\n",
       "      <td>[and, feedback, control, of, thermomechanical,...</td>\n",
       "      <td>[3, 1377, 382, 2, 2360, 603, 4, 5, 5856, 40, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>Harnessing fluctuations in thermodynamic compu...</td>\n",
       "      <td>We experimentally demonstrate that highly stru...</td>\n",
       "      <td>[We, experimentally, demonstrate, that, highly...</td>\n",
       "      <td>[17, 611, 178, 10, 418, 2778, 875, 2, 98, 2819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>Bounding the finite-size error of quantum many...</td>\n",
       "      <td>Finite-size errors (FSEs), the discrepancies b...</td>\n",
       "      <td>[errors, the, between, an, observable, in, a, ...</td>\n",
       "      <td>[2959, 1, 35, 21, 1912, 4, 5, 168, 45, 3, 4, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "0     Cosmological models reconstructed from jerk: A...   \n",
       "1     Investigation into the effect of energy densit...   \n",
       "2     The effects of substrate morphology by regulat...   \n",
       "3     Holographic dual approach to magnetism and mag...   \n",
       "4     Defect induced ferromagnetism in a two-dimensi...   \n",
       "...                                                 ...   \n",
       "2495  Principles of low dissipation computing from a...   \n",
       "2496  Effect of chaos on the simulation of quantum c...   \n",
       "2497  Non-equilibrium quadratic measurement-feedback...   \n",
       "2498  Harnessing fluctuations in thermodynamic compu...   \n",
       "2499  Bounding the finite-size error of quantum many...   \n",
       "\n",
       "                                               Abstract  \\\n",
       "0     The thermodynamic viability of some dark energ...   \n",
       "1     In this study, the effects of volume energy de...   \n",
       "2     When cells are cultured on the micro- or nano-...   \n",
       "3     We propose a dual gravitational theory corresp...   \n",
       "4     Two-dimensional ferromagnetic materials are po...   \n",
       "...                                                 ...   \n",
       "2495  We introduce a thermodynamically consistent, m...   \n",
       "2496  We study how chaos, introduced by a weak pertu...   \n",
       "2497  Measurement and feedback control of thermomech...   \n",
       "2498  We experimentally demonstrate that highly stru...   \n",
       "2499  Finite-size errors (FSEs), the discrepancies b...   \n",
       "\n",
       "                                            Reformatted  \\\n",
       "0     [The, thermodynamic, of, some, dark, energy, m...   \n",
       "1     [In, this, the, effects, of, volume, energy, d...   \n",
       "2     [When, cells, are, on, the, micro, or, structu...   \n",
       "3     [We, propose, a, dual, gravitational, theory, ...   \n",
       "4     [ferromagnetic, materials, are, potential, can...   \n",
       "...                                                 ...   \n",
       "2495  [We, introduce, a, thermodynamically, minimal,...   \n",
       "2496  [We, study, how, introduced, by, a, weak, affe...   \n",
       "2497  [and, feedback, control, of, thermomechanical,...   \n",
       "2498  [We, experimentally, demonstrate, that, highly...   \n",
       "2499  [errors, the, between, an, observable, in, a, ...   \n",
       "\n",
       "                                                 Tokens  \n",
       "0     [8, 18, 2, 171, 989, 31, 77, 109, 1, 574, 164,...  \n",
       "1     [25, 23, 1, 118, 2, 337, 31, 70, 14, 5858, 67,...  \n",
       "2     [713, 1579, 12, 14, 1, 2440, 52, 74, 12, 428, ...  \n",
       "3     [17, 353, 5, 2344, 526, 71, 258, 6, 1064, 1065...  \n",
       "4     [1065, 179, 12, 68, 1464, 10, 27, 24, 1624, 11...  \n",
       "...                                                 ...  \n",
       "2495  [17, 707, 5, 220, 1055, 545, 26, 9, 2746, 1928...  \n",
       "2496  [17, 44, 214, 878, 13, 5, 450, 1114, 1, 2743, ...  \n",
       "2497  [3, 1377, 382, 2, 2360, 603, 4, 5, 5856, 40, 4...  \n",
       "2498  [17, 611, 178, 10, 418, 2778, 875, 2, 98, 2819...  \n",
       "2499  [2959, 1, 35, 21, 1912, 4, 5, 168, 45, 3, 4, 1...  \n",
       "\n",
       "[2500 rows x 4 columns]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### GENERATE SEQUENCES ###\n",
    "##########################\n",
    "training = []\n",
    "labels = []\n",
    "\n",
    "base_length = 35\n",
    "seq_length = base_length + base_length//2\n",
    "\n",
    "data = data.drop(data[data['Tokens'].map(len) < seq_length].index)\n",
    "\n",
    "lengths = [len(sequence) for sequence in data['Tokens']]\n",
    "if min(lengths) >= seq_length:\n",
    "     for sequence in data['Tokens']:\n",
    "        for i in range(seq_length, len(sequence)):\n",
    "            cut = sequence[i - seq_length:i + 1]\n",
    "            training.append(cut[:-1])\n",
    "            labels.append(cut[-1])\n",
    "else:\n",
    "    print(f'The sequence at {lengths.index(min(lengths))} is too short.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training sequence shape is (179412, 52), the training label shape is (179412, 12980)\n",
      "The test sequence shape is (59804, 52), the test label shape is  (59804, 12980)\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### TRAIN/TEST SPLIT ###\n",
    "########################\n",
    "\n",
    "### Shuffle the sets by shuffling a tuple of (sequence, label)\n",
    "\n",
    "compact = list(zip(training,labels))\n",
    "np.random.shuffle(compact)\n",
    "training, labels = zip(*compact)\n",
    "\n",
    "#split into 75% training to 25% test\n",
    "\n",
    "X_train = np.array(training[:int(0.75*len(training))])\n",
    "X_test = np.array(training[int(0.75*len(training)):])\n",
    "\n",
    "y_train_base = np.array(labels)[:int(0.75*len(labels))]\n",
    "y_test_base = np.array(labels)[int(0.75*len(labels)):]\n",
    "\n",
    "y_train = np.zeros((len(y_train_base), num_words), dtype=np.int8)\n",
    "y_test = np.zeros((len(y_test_base), num_words), dtype=np.int8)\n",
    "\n",
    "# One hot encoding of labels\n",
    "for example_index, word_index in enumerate(y_train_base):\n",
    "    y_train[example_index, word_index] = 1\n",
    "\n",
    "for example_index, word_index in enumerate(y_test_base):\n",
    "    y_test[example_index, word_index] = 1\n",
    "\n",
    "# y_train = keras.utils.to_categorical(y_train,num_classes=len(np.unique(y_train)))\n",
    "# y_test = keras.utils.to_categorical(y_test,num_classes=len(np.unique(y_test)))\n",
    "\n",
    "print(f'The training sequence shape is {X_train.shape}, the training label shape is {y_train.shape}')\n",
    "print(f'The test sequence shape is {X_test.shape}, the test label shape is  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 7442 words without pre-trained embeddings.\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### Embedding Matrix ###\n",
    "########################\n",
    "\n",
    "path = 'D:/PythonProjects/PhysicsChatbot/data/glove.6B/glove.6B.100d.txt'\n",
    "glove = np.loadtxt(path, dtype='str', comments=None, encoding= 'UTF-8')\n",
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]\n",
    "del glove\n",
    "\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "\n",
    "not_found = 0\n",
    "\n",
    "for i, word in enumerate(word_lexicon.keys()):\n",
    "    vector = word_lookup.get(word, None)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector\n",
    "    else:\n",
    "        not_found += 1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, len(word_lookup['the'])))\n",
    "\n",
    "not_found = 0\n",
    "\n",
    "for i, word in enumerate(word_lexicon.keys()):\n",
    "    # Look up the word embedding\n",
    "    vector = word_lookup.get(word, None)\n",
    "\n",
    "    # Record in matrix\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector\n",
    "    else:\n",
    "        not_found += 1\n",
    "\n",
    "print(f'There were {not_found} words without pre-trained embeddings.')\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_dir = '../models/'\n",
    "model_name = 'Main_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "SAVE_MODEL = True\n",
    "BATCH_SIZE = 2048\n",
    "VERBOSE = 0\n",
    "EPOCHS = 150\n",
    "\n",
    "\n",
    "def make_callbacks(model_name, save=SAVE_MODEL):\n",
    "    \"\"\"Make list of callbacks for training\"\"\"\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    if save:\n",
    "        callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                f'{model_dir}{model_name}.h5',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False))\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "callbacks = make_callbacks(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 100)         1298000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                42240     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 12980)             1674420   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,022,980\n",
      "Trainable params: 3,022,980\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM_CELLS = 64\n",
    "\n",
    "model = make_word_level_model(\n",
    "    num_words,\n",
    "    embedding_matrix,\n",
    "    lstm_cells=LSTM_CELLS,\n",
    "    trainable=True,\n",
    "    lstm_layers=1)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15768/3878105163.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Concordia Bootcamp\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Concordia Bootcamp\\Python\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Concordia Bootcamp\\Python\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Concordia Bootcamp\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Concordia Bootcamp\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Concordia Bootcamp\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Concordia Bootcamp\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Documents\\Concordia Bootcamp\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Concordia Bootcamp\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=VERBOSE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f83d188c93285744c9e231d0e20b204081cd39f5e44132ec4def0ee68076c04"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
