{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "### UI SHELL ###\n",
    "################\n",
    "\n",
    "from tkinter import *\n",
    "root = Tk()\n",
    "root.title(\"Chatbot\")\n",
    "def send():\n",
    "    send = \"You -> \"+e.get()\n",
    "    txt.insert(END, \"n\"+send)\n",
    "    user = e.get().lower()\n",
    "    if(user == \"hello\"):\n",
    "        txt.insert(END, \"n\" + \"Bot -> Hi\")\n",
    "    elif(user == \"hi\" or user == \"hii\" or user == \"hiiii\"):\n",
    "        txt.insert(END, \"n\" + \"Bot -> Hello\")\n",
    "    elif(e.get() == \"how are you\"):\n",
    "        txt.insert(END, \"n\" + \"Bot -> fine! and you\")\n",
    "    elif(user == \"fine\" or user == \"i am good\" or user == \"i am doing good\"):\n",
    "        txt.insert(END, \"n\" + \"Bot -> Great! how can I help you.\")\n",
    "    else:\n",
    "        txt.insert(END, \"n\" + \"Bot -> Sorry! I dind't got you\")\n",
    "    e.delete(0, END)\n",
    "txt = Text(root)\n",
    "txt.grid(row=0, column=0, columnspan=2)\n",
    "e = Entry(root, width=100)\n",
    "e.grid(row=1, column=0)\n",
    "send = Button(root, text=\"Send\", command=send).grid(row=1, column=1)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "### IMPORTS ###\n",
    "###############\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### Functions ###\n",
    "#################\n",
    "\n",
    "def reformat_text(text):\n",
    "    text = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', text)\n",
    "    text = re.sub(r'\\((\\d+)\\)', r'', text)\n",
    "    text = re.sub(r'\\s\\s', ' ', text)\n",
    "    return text\n",
    "\n",
    "def text_processing(text):\n",
    "    reformatted_text = reformat_text(text)\n",
    "    tokenizer = Tokenizer(filters='\"#$%&*+/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts([reformatted_text])\n",
    "    s = tokenizer.texts_to_sequences([reformatted_text])[0]\n",
    "    s = ' '.join(tokenizer.index_word[i] for i in s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No docs found\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### Obtaining the Data ###\n",
    "##########################\n",
    "# Takes about 25 mins to run, returns 50 000 results\n",
    "\n",
    "token = '7FDWFncezt1eTHOZvbJc78wQkgyC5QuF6Mln1NcD'\n",
    "rows = 100 # fetch 100 records at a time\n",
    "start = 0  # start with the first result\n",
    "abstracts = [] # we'll store the bibcodes of all of our results here\n",
    "titles = []\n",
    "\n",
    "docs = True\n",
    "while docs:\n",
    "    # note that this URL is the same as above, except we've added parameters for start and rows\n",
    "    results = requests.get(\"https://api.adsabs.harvard.edu/v1/search/query?\" \\\n",
    "                       \"q=%20abs%3AThermodynamics\" \\\n",
    "                       \"&sort=date%20desc%2C%20bibcode%20desc\" \\\n",
    "                       \"&fl=title,abstract\", \\\n",
    "                       \"&rows={rows}\" \\\n",
    "                       \"&start={start}\".format(rows=rows,start=start), \\\n",
    "                       headers={'Authorization': 'Bearer ' + token})\n",
    "    try:\n",
    "        docs = results.json()['response']['docs']\n",
    "    except KeyError:\n",
    "        print('No docs found')\n",
    "        break\n",
    "    # pull the bibcodes out of the results into a list\n",
    "    tmp_abstract = [d['abstract'] for d in docs]\n",
    "    tmp_titles = [d['title'] for d in docs]\n",
    "    titles = titles + tmp_titles\n",
    "    abstracts = abstracts + tmp_abstract\n",
    "    start += rows # increment the start value to move to the next page of results\n",
    "\n",
    "print(len(titles))\n",
    "print(len(abstracts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "data = {'Title':titles,'Abstract':abstracts}\n",
    "data = pd.DataFrame(data)\n",
    "data['Title'] = data['Title'].apply(lambda x: x[0])\n",
    "column_values = data[['Title']].values.ravel()\n",
    "unique_values =  pd.unique(column_values)\n",
    "print(len(unique_values))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f83d188c93285744c9e231d0e20b204081cd39f5e44132ec4def0ee68076c04"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
