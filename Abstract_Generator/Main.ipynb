{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "### IMPORTS ###\n",
    "###############\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, TimeDistributed, Dense\n",
    "from tensorflow.keras.layers import Embedding, GRU\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### Functions ###\n",
    "#################\n",
    "\n",
    "def reformat_text(text):\n",
    "    text = re.sub(r'\\(.+?\\)', '', text)\n",
    "    text = re.sub(r'\\{.+?\\}', '', text)\n",
    "    text = re.sub(r'[-_()0-9%$:\\^\\/°\\∼\\~π]', '', text)\n",
    "    text = re.sub(r'(\\.)\\1+','', text)\n",
    "    text = re.sub(r'\\w*<sub>','', text)\n",
    "    text = re.sub(r'\\$.+?\\$', '', text)\n",
    "    text = re.sub(r'<.+?>','',text)\n",
    "    text = re.sub(r'[<>]', '', text)\n",
    "    text = re.sub(r'(\\s)\\1+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def lookup(param):\n",
    "    if type(param) is int:\n",
    "        return word_index[param]\n",
    "    elif type(param) is str:\n",
    "        return word_lexicon[param]\n",
    "    else:\n",
    "        print('Parameter not accepted.')\n",
    "\n",
    "def shuffle(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def remove_words(arr,lexicon):\n",
    "    arr = arr.split(' ')\n",
    "\n",
    "    for token in arr:\n",
    "        if token not in lexicon:\n",
    "            arr.remove(token)\n",
    "            \n",
    "    return arr\n",
    "\n",
    "def convert_back(arr):\n",
    "    sentence = []\n",
    "\n",
    "    for token in arr:\n",
    "        sentence.append(lookup(token))\n",
    "\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "def least_used_words(word_counts,min_freq):\n",
    "    delwords = []\n",
    "\n",
    "    for word in word_counts:\n",
    "        if word_counts[word] <= min_freq:\n",
    "            delwords.append(word)\n",
    "\n",
    "    return delwords\n",
    "\n",
    "def make_model(num_words,\n",
    "                          embedding_matrix,\n",
    "                          lstm_cells=64,\n",
    "                          trainable=False,\n",
    "                          lstm_layers=1,\n",
    "                          bi_direc=False):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Map words to an embedding\n",
    "    if not trainable:\n",
    "        model.add(\n",
    "            Embedding(\n",
    "                input_dim=num_words,\n",
    "                output_dim=embedding_matrix.shape[1],\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=False,\n",
    "                mask_zero=True))\n",
    "        model.add(Masking())\n",
    "    else:\n",
    "        model.add(\n",
    "            Embedding(\n",
    "                input_dim=num_words,\n",
    "                output_dim=embedding_matrix.shape[1],\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=True))\n",
    "\n",
    "    # If want to add multiple LSTM layers\n",
    "    if lstm_layers > 1:\n",
    "        for i in range(lstm_layers - 1):\n",
    "            model.add(\n",
    "                LSTM(\n",
    "                    lstm_cells,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0.1,\n",
    "                    recurrent_dropout=0.1))\n",
    "\n",
    "    # Add final LSTM cell layer\n",
    "    if bi_direc:\n",
    "        model.add(\n",
    "            Bidirectional(\n",
    "                LSTM(\n",
    "                    lstm_cells,\n",
    "                    return_sequences=False,\n",
    "                    dropout=0.1,\n",
    "                    recurrent_dropout=0.1)))\n",
    "    else:\n",
    "        model.add(\n",
    "            LSTM(\n",
    "                lstm_cells,\n",
    "                return_sequences=False,\n",
    "                dropout=0.1,\n",
    "                recurrent_dropout=0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def load_and_evaluate(model_name):\n",
    "\n",
    "    model = load_model(f'{model_dir}{model_name}.h5')\n",
    "    r = model.evaluate(X_test, y_test, batch_size=2048, verbose=1)\n",
    "\n",
    "    valid_crossentropy = r[0]\n",
    "    valid_accuracy = r[1]\n",
    "\n",
    "    print(f'Cross Entropy: {round(valid_crossentropy, 4)}')\n",
    "    print(f'Accuracy: {round(100 * valid_accuracy, 2)}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### Obtaining the Data ###\n",
    "##########################\n",
    "\n",
    "token = 'N/A' #Available by request\n",
    "rows = 1\n",
    "start = 0\n",
    "abstracts = []\n",
    "titles = []\n",
    "\n",
    "docs = True\n",
    "while docs:\n",
    "    result = requests.get(\"https://api.adsabs.harvard.edu/v1/search/query?\" \\\n",
    "                       \"q=%20abs%3AThermodynamics\" \\\n",
    "                       \"&sort=date%20desc%2C%20bibcode%20desc\" \\\n",
    "                       \"&fl=title,abstract\", \\\n",
    "                       \"&rows={rows}\" \\\n",
    "                       \"&start={start}\".format(rows=rows,start=start), \\\n",
    "                       headers={'Authorization': 'Bearer ' + token})\n",
    "#If the qeury is successful\n",
    "    if result.status_code == 200:\n",
    "        docs = result.json()['response']['docs']\n",
    "        if 'abstract' in docs[0] and 'title' in docs[0]:\n",
    "            tmp_abstract = [d['abstract'] for d in docs]\n",
    "            tmp_titles = [d['title'] for d in docs]\n",
    "            titles = titles + tmp_titles\n",
    "            abstracts = abstracts + tmp_abstract\n",
    "    elif result.status_code == 429:\n",
    "        print(f'Collected {len(titles)} samples.')\n",
    "        break\n",
    "    start += rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "### COPY LISTS ###\n",
    "##################\n",
    "CT = titles[:]\n",
    "CA = abstracts[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### INSPECT Data ###\n",
    "####################\n",
    "\n",
    "data = {'Title':CT,'Abstract':CA}\n",
    "data = pd.DataFrame(data)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>Phase Transformation Modeling for Hypo Peritec...</td>\n",
       "      <td>Phase change of steel during cooling affects t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4405</th>\n",
       "      <td>Single Crystal Elasticity of MgSiO&lt;SUB&gt;3&lt;/SUB&gt;...</td>\n",
       "      <td>The combination of seismic observations and mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>Theoretical Interpretation of Thermophysical P...</td>\n",
       "      <td>In continuation to our previous publication, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>Scandium decorated C$_{24}$ fullerene as high ...</td>\n",
       "      <td>Using first-principles density functional theo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>Modified strong-coupling treatment of a spin-1...</td>\n",
       "      <td>A quantum spin-1/2 antiferromagnetic Heisenber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>E&lt;SUP&gt;2&lt;/SUP&gt; and gamma distributions in polyg...</td>\n",
       "      <td>From solar supergranulation to salt flats in B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3954</th>\n",
       "      <td>Disinfection By-Product Removal by Activated C...</td>\n",
       "      <td>This research was aimed to study the efficienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>Collective variable-based enhanced sampling an...</td>\n",
       "      <td>Collective variable-based enhanced sampling me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3207</th>\n",
       "      <td>Study on variation of thermodynamic parameters...</td>\n",
       "      <td>To study the variation regularities of the mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3614</th>\n",
       "      <td>Contribution of electronic entropy to the orde...</td>\n",
       "      <td>Cu&lt;SUB&gt;3&lt;/SUB&gt;Au experiences a phase transitio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "3187  Phase Transformation Modeling for Hypo Peritec...   \n",
       "4405  Single Crystal Elasticity of MgSiO<SUB>3</SUB>...   \n",
       "928   Theoretical Interpretation of Thermophysical P...   \n",
       "615   Scandium decorated C$_{24}$ fullerene as high ...   \n",
       "4238  Modified strong-coupling treatment of a spin-1...   \n",
       "1255  E<SUP>2</SUP> and gamma distributions in polyg...   \n",
       "3954  Disinfection By-Product Removal by Activated C...   \n",
       "1565  Collective variable-based enhanced sampling an...   \n",
       "3207  Study on variation of thermodynamic parameters...   \n",
       "3614  Contribution of electronic entropy to the orde...   \n",
       "\n",
       "                                               Abstract  \n",
       "3187  Phase change of steel during cooling affects t...  \n",
       "4405  The combination of seismic observations and mi...  \n",
       "928   In continuation to our previous publication, n...  \n",
       "615   Using first-principles density functional theo...  \n",
       "4238  A quantum spin-1/2 antiferromagnetic Heisenber...  \n",
       "1255  From solar supergranulation to salt flats in B...  \n",
       "3954  This research was aimed to study the efficienc...  \n",
       "1565  Collective variable-based enhanced sampling me...  \n",
       "3207  To study the variation regularities of the mas...  \n",
       "3614  Cu<SUB>3</SUB>Au experiences a phase transitio...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'D:\\PythonProjects\\AbstractGenerator\\data\\data.csv')\n",
    "data = data.drop('Unnamed: 0',axis=1)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database contains 99.41398074508162% unique entries.\n",
      " With 36919 unique words\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### Initial formatting ###\n",
    "##########################\n",
    "\n",
    "if type(data.iloc[0,0]) == list: \n",
    "    data['Title'] = data['Title'].apply(lambda x: x[0])\n",
    "\n",
    "\n",
    "column_values = data[['Title']].values.ravel()\n",
    "unique_values =  pd.unique(column_values)\n",
    "\n",
    "#Tokenizer on the entire dataset\n",
    "\n",
    "data['Reformatted'] = data['Abstract'].apply(reformat_text)\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(data['Reformatted'])\n",
    "data['Tokens'] = tokenizer.texts_to_sequences(data['Reformatted'])\n",
    "\n",
    "word_lexicon = tokenizer.word_index\n",
    "word_index = tokenizer.index_word\n",
    "num_words = len(word_lexicon) + 1\n",
    "word_counts = tokenizer.word_counts\n",
    "sorted_counts = dict(sorted(dict(word_counts).items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "\n",
    "print(f'The database contains {len(unique_values)/data.shape[0]*100}% unique entries.\\n With {num_words} unique words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database contains 99.41398074508162% unique entries.\n",
      " With 17794 unique words\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### Further Formatting ###\n",
    "##########################\n",
    "\n",
    "### Removing the lowest used words.\n",
    "delwords = least_used_words(word_counts=word_counts,min_freq=15)\n",
    "\n",
    "for word in delwords:\n",
    "    del word_lexicon[word]\n",
    "\n",
    "data['Reformatted'] = data['Reformatted'].apply(lambda x : remove_words(x,word_lexicon))\n",
    "\n",
    "#Running tokenizer again to update the word lexicon to be used in the model\n",
    "\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(data['Reformatted'])\n",
    "data['Tokens'] = tokenizer.texts_to_sequences(data['Reformatted'])\n",
    "\n",
    "word_lexicon = tokenizer.word_index\n",
    "word_index = tokenizer.index_word\n",
    "num_words = len(word_lexicon) + 1\n",
    "word_counts = tokenizer.word_counts\n",
    "sorted_counts = dict(sorted(dict(word_counts).items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "\n",
    "print(f'The database contains {len(unique_values)/data.shape[0]*100}% unique entries.\\n With {num_words} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### GENERATE SEQUENCES ###\n",
    "##########################\n",
    "training = []\n",
    "labels = []\n",
    "\n",
    "base_length = 35\n",
    "seq_length = base_length*2\n",
    "\n",
    "data = data.drop(data[data['Tokens'].map(len) < seq_length].index)\n",
    "\n",
    "#CUtting the tokens into sequences and adding them to an array, here every 36 words in our token sequence forms\n",
    "# a training label pair from the begging to the end of the token sequence. \n",
    "\n",
    "lengths = [len(sequence) for sequence in data['Tokens']]\n",
    "if min(lengths) >= seq_length:\n",
    "     for sequence in data['Tokens']:\n",
    "        for i in range(seq_length, len(sequence)):\n",
    "            cut = sequence[i - seq_length:i + 1]\n",
    "            training.append(cut[:-1])\n",
    "            labels.append(cut[-1])\n",
    "else: #Not expected to be used but here to avoid an error\n",
    "    print(f'The sequence at {lengths.index(min(lengths))} is too short.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training sequence shape is (287175, 70), the training label shape is (287175, 17794)\n",
      "The test sequence shape is (95725, 70), the test label shape is  (95725, 17794)\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### TRAIN/TEST SPLIT ###\n",
    "########################\n",
    "\n",
    "### Shuffle the sets by shuffling a tuple of (sequence, label)\n",
    "\n",
    "compact = list(zip(training,labels))\n",
    "np.random.shuffle(compact)\n",
    "training, labels = zip(*compact)\n",
    "\n",
    "#split into 75% training to 25% test\n",
    "\n",
    "X_train = np.array(training[:int(0.75*len(training))])\n",
    "X_test = np.array(training[int(0.75*len(training)):])\n",
    "\n",
    "y_train_base = np.array(labels)[:int(0.75*len(labels))]\n",
    "y_test_base = np.array(labels)[int(0.75*len(labels)):]\n",
    "\n",
    "y_train = np.zeros((len(y_train_base), num_words), dtype=np.int8)\n",
    "y_test = np.zeros((len(y_test_base), num_words), dtype=np.int8)\n",
    "\n",
    "# One hot encoding of labels\n",
    "for example_index, word_index in enumerate(y_train_base):\n",
    "    y_train[example_index, word_index] = 1\n",
    "\n",
    "for example_index, word_index in enumerate(y_test_base):\n",
    "    y_test[example_index, word_index] = 1\n",
    "\n",
    "print(f'The training sequence shape is {X_train.shape}, the training label shape is {y_train.shape}')\n",
    "print(f'The test sequence shape is {X_test.shape}, the test label shape is  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 10879 words without pre-trained embeddings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17794, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################\n",
    "### Embedding Matrix ###\n",
    "########################\n",
    "\n",
    "path = 'D:/PythonProjects/AbstractGenerator/data/glove.6B/glove.6B.100d.txt'\n",
    "glove = np.loadtxt(path, dtype='str', comments=None, encoding= 'UTF-8')\n",
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]\n",
    "del glove\n",
    "\n",
    "#set up embedding matrix\n",
    "\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, len(word_lookup['the'])))\n",
    "\n",
    "not_found = 0\n",
    "\n",
    "for i, word in enumerate(word_lexicon.keys()):\n",
    "    # Look up the word embedding\n",
    "    vector = word_lookup.get(word, None)\n",
    "\n",
    "    # Record in matrix\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector\n",
    "    else:\n",
    "        not_found += 1\n",
    "\n",
    "print(f'There were {not_found} words without pre-trained embeddings.')\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'D:/PythonProjects/AbstractGenerator/models/'\n",
    "model_name = 'Main_model'\n",
    "SAVE_MODEL = True\n",
    "BATCH_SIZE = 2048\n",
    "VERBOSE = 0\n",
    "EPOCHS = 150\n",
    "\n",
    "#Run the model until the accuracy does not improve significantly and save that model.\n",
    "\n",
    "def make_callbacks(model_name, save=SAVE_MODEL):\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    if save:\n",
    "        callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                f'{model_dir}{model_name}.h5',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False))\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "callbacks = make_callbacks(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         1779400   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                42240     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 17794)             2295426   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,125,386\n",
      "Trainable params: 4,125,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Create the model\n",
    "\n",
    "LSTM_CELLS = 64\n",
    "\n",
    "model = make_model(\n",
    "    num_words,\n",
    "    embedding_matrix,\n",
    "    lstm_cells=LSTM_CELLS,\n",
    "    trainable=True,\n",
    "    lstm_layers=1)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile and run the model, it takes around 80mins on my laptop.\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=VERBOSE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best model and test the accuracy\n",
    "\n",
    "model_dir = r'D:/PythonProjects/AbstractGenerator/deployment/models/'\n",
    "model_name = 'Main_model'\n",
    "model = load_model(f'{model_dir}{model_name}.h5')\n",
    "model = load_and_evaluate(model_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f83d188c93285744c9e231d0e20b204081cd39f5e44132ec4def0ee68076c04"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
